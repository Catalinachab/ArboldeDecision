---
output: pdf_document
---

\newpage
\begin{titlepage}
  \centering
  \vspace*{1cm}
  
  {\Huge \textbf{TD VI: Inteligencia Artificial}}\\[0.5cm]
  
  {\Large \textbf{Trabajo Práctico I}}\\[0.5cm]
  
  {\large \textbf{Universidad Torcuato Di Tella}}\\[1.5cm]
  
  {\Large \textbf{Participantes:}}\\[0.5cm]
  
  {\Large Catalina Brusco, Catalina Chab López y Belén Chen}\\[0.5cm]
  
  \vfill
  
  {\Large Fecha: Abril del 2025}\\[0.5cm]
\end{titlepage}

# **1. Introducción al problema: origen y variables principales del conjunto de datos**


El conjunto de datos elegido proviene de Kaggle y se utiliza para predecir si un préstamo solicitado por una persona será aprobado o rechazado, basándose en distintas características del solicitante. El conjunto fue enriquecido con variables adicionales basadas en *Riesgo Financiero* para la Aprobación de Préstamos. Además, se aplicó *SMOTENC (Synthetic Minority Over-sampling Technique for Nominal and Continuous)* para generar nuevos puntos de datos y ampliar el conjunto de instancias. El conjunto de datos contiene 45,000 registros y 14 variables. A continuación se describen las variables clave:

```{r, echo=FALSE, warning=FALSE,  message=FALSE}
library(knitr)

# Definir los datos
tabla <- data.frame(
  Columna = c("person_age", "person_gender", "person_education", "person_income",
              "person_emp_exp", "person_home_ownership", "loan_amnt", "loan_intent",
              "loan_int_rate", "loan_percent_income", "cb_person_cred_hist_length",
              "credit_score", "previous_loan_defaults_on_file", "loan_status (objetivo)"),
  Descripción = c("Edad de la persona", "Género de la persona", "Nivel de educación más alto alcanzado",
                  "Ingreso anual de la persona", "Años de experiencia laboral",
                  "Estado de propiedad de la vivienda (ej., alquiler, propia)",
                  "Monto del préstamo solicitado", "Propósito del préstamo",
                  "Tasa de interés del préstamo", "Monto del préstamo como porcentaje del ingreso anual",
                  "Longitud del historial de crédito en años", "Puntaje de crédito de la persona",
                  "Indicador de los anteriores incumplimientos de préstamo",
                  "Estado del préstamo: 1 = aprobado; 0 = rechazado"),
  `Tipo de Dato` = c("Float", "Categórico", "Categórico", "Float",
                     "Entero", "Categórico", "Float", "Categórico",
                     "Float", "Float", "Float", "Entero",
                     "Categórico", "Entero (Binario)")
)

# Generar la tabla en formato LaTeX
kable(tabla, format = "latex", booktabs = TRUE, align = "l",
      caption = "Resumen de los atributos del dataset", longtable = TRUE)


```

Utilizar un árbol de decisión para modelar este problema es adecuado por varias razones. Primero, permite realizar tareas de clasificación binaria, como predecir si un préstamo será aprobado o rechazado, ya que divide los datos en función de las características más importantes. Segundo, maneja eficientemente datos mixtos, es decir, tanto numéricos (como ingresos y puntajes de crédito) como categóricos (como género y estado civil), sin necesidad de transformaciones complicadas. Tercero, la estructura de un árbol de decisión permite dividir los datos en subgrupos homogéneos, lo que ayuda a identificar clientes con características similares que pueden tener un comportamiento parecido, como los que tienen un puntaje de crédito bajo o altos ingresos. Esto es importante porque las relaciones entre características, como ingresos y puntaje de crédito, no siempre son lineales, y los árboles pueden manejar estas interacciones. Finalmente, su gran ventaja es la interpretabilidad; los árboles de decisión son fáciles de entender y explican claramente las razones detrás de cada clasificación, lo cual es fundamental en el ámbito bancario, donde se necesitan justificar las decisiones de aprobación o rechazo de préstamos.

# **2. Preparación de los datos**

Es necesario preprocesar los datos, para lo cual realizaremos un análisis que incluya la verificación de las variables y su clasificación, la normalización o escalado de las variables, la detección de valores faltantes, la identificación de posibles anomalías y balanceado de datos.

Como se mencionó anteriormente, las variables están correctamente clasificadas según su tipo (categórico o numérico). Dado que vamos a utilizar árboles de decisión, no es necesario normalizar ni escalar los datos, ya que este modelo no depende de las magnitudes de las variables. Los árboles de decisión dividen los datos basándose en los valores específicos de las características, por lo que la escala de las variables no influye en el desempeño del modelo. Asimismo, detallaremos que no existen valores faltantes en el conjunto de datos, como se puede observar en la Tabla 2.

```{r, echo=FALSE, warning=FALSE,  message=FALSE}

data <- read.csv("loan_data.csv")
nulls <- colSums(is.na(data))

cantidad_atributos <- length(nulls)
atributos_cero_nulos <- sum(nulls == 0)

tabla_resumen <- data.frame(
  `Cantidad de atributos verificados` = cantidad_atributos,
  `Atributos con 0 nulos` = atributos_cero_nulos
)

library(knitr)
kable(tabla_resumen, align = "c", caption = "Resumen de valores nulos")

```

También corroboramos que no huebiese una cantidad excesiva de filas repetidas, de hecho, nos dió cero.

```{r, echo=FALSE}
duplicados<-duplicated(data)
cant<-sum(duplicados)
```


Habiendo mencionado lo anterior, continuaremos con el análisis de las distribuciones de los distintos atributos. Como se puede observar en la Figura 1, se aplicó una transformación logarítmica a tres atributos clave: Edad, Ingresos e Historial de Crédito. Esta transformación es relevante porque, al ser creciente, no altera la distribución subyacente de los datos, pero sí comprime el rango de los valores grandes y expande el de los valores pequeños, lo que facilita su análisis.

Es importante destacar la relevancia de analizar la distribución de cada atributo. En este caso, observamos que atributos como Edad, Monto del Préstamo e Ingresos presentan una distribución sesgada a la derecha, mientras que el Puntaje de Crédito está sesgado a la izquierda. Esta asimetría podría generar predicciones erróneas para los valores extremos en cada uno de estos atributos. Además, notamos que el Historial de Crédito no sigue una distribución bien definida, lo que añade complejidad a su análisis.

## **Distribución de Variables Numéricas**

```{r, echo=FALSE, warning=FALSE, fig.cap="Distribución de Variables Numéricas",  message=FALSE}
loan_data <- data
loan_data$log_person_age <- log1p(loan_data$person_age)
loan_data$log_person_income <- log1p(loan_data$person_income)
loan_data$log_cb_person_cred_hist_length <- log1p(loan_data$cb_person_cred_hist_length)
par(mfrow=c(2,3))

hist(loan_data$log_person_age, 
     main = "Log Edad", 
     xlab = "log(1 + Edad)", 
     col = "lightblue", 
     border = "black")


hist(loan_data$log_person_income, 
     main = "Log Ingresos", 
     xlab = "log(1 + Ingresos)", 
     col = "lightblue", 
     border = "black")
hist(loan_data$loan_amnt, main="Monto del Préstamo", xlab="Monto", col="lightblue")
hist(loan_data$loan_int_rate, main="Tasa de Interés", xlab="Porcentaje", col="lightblue")
hist(loan_data$credit_score, main="Puntaje de Crédito", xlab="Score", col="lightblue")

hist(loan_data$log_cb_person_cred_hist_length, 
     main = "Log Historial de Crédito", 
     xlab = "log(1 + Años)", 
     col = "lightblue", 
     border = "black")
par(mfrow=c(1,1))

```
\newpage

## **Distribución de Variables Categóricas**

Luego de analizar la distribución de las variables categóricas, no se identifican patrones relevantes ni desequilibrios significativos que requieran mención.
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=14, fig.height=8}
library(ggplot2)
library(dplyr)
library(patchwork)

# cat_vars <- c("person_gender", 
#               "person_education", 
#               "person_home_ownership", 
#               "loan_intent", 
#               "previous_loan_defaults_on_file", 
#               "loan_status")
# 
# plots <- lapply(cat_vars, function(var) {
#   ggplot(loan_data, aes_string(x = var)) +
#     geom_bar(fill = "lightblue") +
#     ggtitle(paste("Distribución de", var)) +
#     theme_minimal() +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1),
#           plot.title = element_text(size = 10))
# })
# 
# # Armar las filas
# fila1 <- plots[[1]] + plots[[2]] + plots[[3]] + plot_layout(ncol = 3)
# fila2 <- plots[[4]] + plots[[5]] + plots[[6]] + plot_layout(ncol = 3)
# 
# # Combinar filas con un espacio
# fila1 / plot_spacer() / fila2 + 
#   plot_layout(heights = c(1, 0.2, 1))  # El 0.2 define el espacio entre filas

```


## **Anomalías** 

```{r, echo=FALSE, warning=FALSE, fig.cap="Boxplots de Experiencia Laboral y Edad", message=FALSE}
par(mfrow = c(1, 2))  # 1 fila y 2 columnas
  
  # Boxplot para la variable 'person_age'
  boxplot(data$person_age, 
          main = "Distribución de la Edad", 
          ylab = "Edad", 
          col = "lightblue", 
          border = "black", 
          horizontal = FALSE)  # gráfico vertical
  
  # Boxplot para la variable 'person_emp_exp'
  boxplot(data$person_emp_exp, 
          main = "Distribución de la Experiencia Laboral", 
          ylab = "Años de Experiencia Laboral", 
          col = "lightblue", 
          border = "black", 
          horizontal = FALSE)  # gráfico vertical
  
```

En el análisis preliminar de este conjunto de datos, se han identificado algunos valores atípicos (outliers) en los atributos de "Edad" (como personas con más de 120 años) y "Experiencia Laboral" (casos donde había más de 100 años de experiencia laboral). Sin embargo, su cantidad es chica (7 casos en un total de 45,000), lo que sugiere que no tendrán un impacto significativo en el entrenamiento del modelo. También, la distribución de la media en ambos Boxplots es bastante concentrada, lo que indica que la presencia de estos outliers no afecta la distribución general de los datos. En resumen, debido a que su número es tan pequeño, su impacto es prácticamente nulo, por lo que no es necesario eliminar estas filas. 


Como resultado de los análisis realizados, no se eliminará ninguna información del conjunto de datos para evitar introducir sesgos en el proceso.

## **Chequeo de datos balanceados** 

Para evaluar el balance de nuestro conjunto de datos, analizamos las proporciones de las categorías de la variable de interés, en este caso, el estado del préstamo (aceptado o rechazado). A continuación, creamos un gráfico de barras que muestra la proporción de "Aceptado" y "Rechazado" en el conjunto de datos.
  

```{r fig.width=4, fig.height=3, fig.cap="Proporción de Aceptados y Rechazados", echo=FALSE, warning=FALSE, message=FALSE}
# Contar proporciones
yes_no_counts <- table(data$loan_status)
yes_no_proportions <- prop.table(yes_no_counts)

# Ajustar márgenes y reducir tamaño
par(mar = c(3, 3, 2, 1), mgp = c(2, 0.5, 0), tck = -0.02)

# Crear el gráfico de barras más compacto
barplot(yes_no_proportions, 
        main = "",  # Quitamos el título para agregarlo con title()
        xlab = "Estado del Préstamo", 
        ylab = "Proporción", 
        col = c("lightcoral", "lightblue"), 
        border = "black", 
        names.arg = c("Rechazado", "Aceptado"),  
        ylim = c(0, 1),  
        las = 1,        
        cex.names = 0.7, # Reducir tamaño de etiquetas
        cex.axis = 0.7,  # Reducir tamaño de los ejes
        space = 0.3)    

# Agregar título con tamaño reducido
title(main = "Proporción de Aceptados y Rechazados", cex.main = 0.8)



```

Al examinar el gráfico y las proporciones obtenidas, observamos que aproximadamente el 22% de los registros corresponden a "Aceptados" y el 77% a "Rechazados". Esto indica que el conjunto de datos no está completamente balanceado, aunque la desproporción no es extremadamente alta.
La desproporción en el conjunto de datos podría generar un sesgo hacia la clase mayoritaria ("Rechazados"), lo que podría resultar en un mejor desempeño del modelo para predecir esta clase y un desempeño inferior para la clase minoritaria ("Aceptados").
  
Sin embargo, decidimos no intervenir en el conjunto de datos eliminando registros o utilizando técnicas de balanceo como sobremuestreo o submuestreo. Esto se debe a que tales técnicas podrían introducir sesgos adicionales o alterar la representatividad del conjunto de datos. En cambio, optamos por mantener la estructura original y tener en cuenta las proporciones de las clases al entrenar, validar y evaluar el modelo.


# **3. Construcción de árbol de decisión básico**


```{r, echo=FALSE, warning=FALSE, message=FALSE}

library(caret)
library(rpart)
library(rpart.plot)

# Fijar semilla para replicabilidad
set.seed(123)

# Crear índices estratificados para entrenamiento (70%)
train_index <- createDataPartition(data$loan_status, p = 0.70, list = FALSE)

# Conjunto de entrenamiento
train_data <- data[train_index, ]

# Resto de los datos (30% para validación y testeo)
remaining <- data[-train_index, ]

# Dividir el 30% restante en validación (15%) y testeo (15%), manteniendo la proporción de loan_status
val_index <- createDataPartition(remaining$loan_status, p = 0.50, list = FALSE)
val_data <- remaining[val_index, ]
test_data <- remaining[-val_index, ]

# Verificar la distribución de loan_status en cada conjunto
#prop.table(table(train_data$loan_status))  # Proporción en entrenamiento
#prop.table(table(val_data$loan_status))    # Proporción en validación
#prop.table(table(test_data$loan_status))   # Proporción en testeo

# Imprimir tamaños
#cat("Tamaño de Entrenamiento:", nrow(train_data), "\n")
#cat("Tamaño de Validación:", nrow(val_data), "\n")
#cat("Tamaño de Testeo:", nrow(test_data), "\n")



tree <- rpart(formula = loan_status ~ person_age + person_gender + person_education + person_income + person_emp_exp + person_home_ownership + loan_amnt + loan_intent + loan_int_rate + loan_percent_income + cb_person_cred_hist_length + credit_score + previous_loan_defaults_on_file, 
              data = train_data, 
              method = "class")

```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Árbol de Decisión del Modelo"}
rpart.plot(tree, main = "Árbol de Decisión", type = 2, extra = 104)

```

Al realizar el análisis, mantuvimos la proporción de otorgamiento de préstamos (22%) y rechazo (78%) a lo largo de las etapas de entrenamiento, validación y testeo. En la descripción del árbol de decisión, observamos que cada nodo se compone de tres filas:

1.La primera fila indica la categoría de "no" (rechazo del préstamo).

2.La segunda fila muestra la proporción de casos dentro del subgrupo.

3.La tercera fila indica cuántos datos se concentran en ese nivel del árbol.

El árbol de decisión muestra que el primer factor que se utiliza para determinar si se le otorga un préstamo es la existencia de defaults previos (previous_loan_defaults_on_file), donde la presencia de antecedentes implica una altísima probabilidad de no pago. En ausencia de defaults, el siguiente factor relevante es la relación entre el monto solicitado y el ingreso (loan_percent_income), siendo los valores altos indicativos de mayor riesgo. Además, la tasa de interés del préstamo (loan_int_rate) cumple un rol clave: tasas superiores al 14% incrementan significativamente la probabilidad de incumplimiento. El nivel de ingresos (person_income) también influye, ya que a menor ingreso, el riesgo aumenta, especialmente combinado con tasas altas. La intención del préstamo (loan_intent), cuando es para educación, fines personales o emprendimientos, aparece asociada a mayor riesgo. Finalmente, la tenencia de vivienda propia o bajo hipoteca (person_home_ownership) contribuye a elevar la probabilidad de default en combinación con tasas altas y un elevado porcentaje préstamo/ingreso. En conjunto, estas variables permiten identificar perfiles de alto riesgo de manera clara. 

Como conclusión, utilizando esta instancia del modelo entrenado con el conjunto de datos train se le rechazaría al 83% el préstamo y al resto se le otorgaría. 

## **Hiperprámetros por defecto del árbol**

```{r, echo=FALSE, results="hide"}
tree$control
```

A continuación se detallan los hiperparámetros utilizados por defecto en la construcción del árbol y el impacto que tiene cada uno en la estructura del modelo:

minsplit = 20: Define el tamaño mínimo de observaciones en un nodo para que el árbol considere realizar una partición. Un valor de 20 limita las divisiones a grupos con un tamaño suficientemente grande, evitando sobreajuste en nodos con pocos datos.

minbucket = 7: Indica el tamaño mínimo que pueden tener las hojas terminales. Esto asegura que cada hoja final contenga al menos 7 observaciones, promoviendo estabilidad en las predicciones.

cp = 0.01: Es el parámetro de complejidad que regula el proceso de poda. Solo se aceptan divisiones que logren mejorar la calidad del ajuste en al menos un 1%. Un valor de 0.01 representa un control intermedio, balanceando entre un árbol complejo y uno demasiado simple.

maxcompete = 4: Almacena hasta 4 splits “competidores” cercanos al mejor split en cada nodo. Esto es útil para analizar qué otras variables casi logran ser seleccionadas.

maxsurrogate = 5: Controla la cantidad máxima de variables sustitutas utilizadas cuando hay datos faltantes en la variable principal de división. La presencia de 5 surrogates mejora la capacidad del modelo para manejar datos incompletos. En nuestro caso, al no haber datos faltantes, este hiperparámetro pasa a ser irrelevante. 

usesurrogate = 2: Define cómo se utilizan los surrogates. Con un valor de 2, el árbol emplea surrogate splits incluso si la variable principal está disponible, siempre que aporten mejora en el ajuste.

surrogatestyle = 0: Indica que la selección de variables sustitutas se realiza utilizando un índice de concordancia simple, priorizando velocidad y simplicidad.

maxdepth = 30: Fija la profundidad máxima que puede alcanzar el árbol. Un valor alto de 30 permite que, si los datos y el pruning lo permiten, el árbol tenga una gran profundidad.

xval = 10: Determina que la validación cruzada interna para el proceso de poda se realice en 10 particiones, aportando mayor robustez a la selección del tamaño óptimo del árbol.


# **4. Evaluación del árbol de decisión básico** 

Se construye una matriz de confusión para analizar el rendimiento del modelo de clasificación. Inicialmente, se determina el número de predicciones positivas y negativas en el conjunto de prueba. A continuación, se define la función gen_m_c, que recibe como parámetros las etiquetas reales y las predicciones del modelo.Finalmente, los valores de la matriz se ajustan para representar la proporción de aciertos y errores, obteniendo así una versión normalizada de la matriz de confusión.

1. *TP* (Verdaderos Positivos): 75.7% 
2. *TN* (Verdaderos Negativos): 15.26% 
3. *FP* (Falsos Positivos): 6.98%
4. *FN* (Falsos Negativos): 2.06% 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Realizar predicciones en el conjunto de testeo
predicciones_prob <- predict(tree, newdata = test_data, type = "prob")
predicciones_clase <- predict(tree, newdata = test_data, type = "class")

# Ver las probabilidades predichas
# head(predicciones_prob)

# Ver las clases predichas
# head(predicciones_clase)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(pROC)        # Para calcular AUC-ROC
library(pheatmap)
library(gridExtra)
library(grid)

# Crear matriz de confusión
matriz_conf <- confusionMatrix(predicciones_clase, as.factor(test_data$loan_status))

# Normalizar la matriz en porcentajes
total <- sum(matriz_conf$table)
matriz_conf$table <- (matriz_conf$table / total) * 100  
gen_m_c = function(matriz_conf, filename = NULL) {
  # Crear el heatmap sin imprimir automáticamente
  p <- pheatmap(matriz_conf, 
                display_numbers = TRUE,
                color = colorRampPalette(c("coral", "gold"))(50),
                main = "Confusion Matrix",
                number_format = "%.2f",
                cluster_rows = FALSE,
                cluster_cols = FALSE,
                legend = TRUE,  
                fontsize = 12,       
                fontsize_number = 12, 
                cellwidth = 60,       
                cellheight = 60,
                silent = TRUE)  # Evita que `pheatmap` imprima automáticamente


  # Combinar heatmap y título en un solo gráfico
  grid.newpage() # Crear una nueva página para evitar superposición
  grid.draw(p$gtable)  # Dibujar solo una vez

  # Guardar la imagen si se especifica un filename
  if (!is.null(filename)) {
    ggsave(filename, plot = p$gtable, width = 6, height = 6)
  }
}

# Llamar a la función con la matriz corregida
gen_m_c(matriz_conf$table, filename = "matriz_confusion.png")


```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
library(caret)

# Suponiendo que tienes las predicciones y el conjunto de datos de test
# Confusión matriz
matriz_conf <- confusionMatrix(predicciones_clase, as.factor(test_data$loan_status))
total <- sum(matriz_conf$table)

# Extraer los valores de la matriz de confusión y guardar los porcentajes que representan
TP <- matriz_conf$table[1, 1] / total * 100 # Verdaderos positivos
TN <- matriz_conf$table[2, 2] / total * 100 # Verdaderos negativos
FP <- matriz_conf$table[1, 2] / total * 100 # Falsos positivos
FN <- matriz_conf$table[2, 1] / total * 100 # Falsos negativos

```

Los resultados obtenidos de las métricas de evaluación se detallan a continuación:

1. **Accuracy (Precisión Global)**: 0.9096
   - Esto significa que el modelo clasificó correctamente el 90.96% de todas las instancias en el conjunto de datos (tanto positivas como negativas). 

2. **Precision (Precisión)**: 0.9156
   - Este valor indica que, de todas las predicciones que el modelo identificó como positivas (FP and TP), el 91.56% fueron efectivamente positivas. Como es un valor alto de precisión, esto significa que hay pocos falsos positivos.

3. **Recall (Sensibilidad o Exhaustividad)**: 0.9735
   -  Esto significa que el modelo fue capaz de identificar correctamente el 97.35% de todas las instancias positivas reales (FN y TP) en el conjunto de datos. Como es un valor alto de recall, esto significa que el modelo tiene una buena capacidad para identificar casos positivos, aunque pueda tener algunos falsos positivos.

4. **F1-score**: 0.9437
   - El F1-score es 0.9437 y es el promedio ponderado de la precisión y el recall.Este valor es especialmente útil cuando hay un desbalance en las clases, ya que captura tanto la exactitud como la exhaustividad del modelo. Como es un F1-score alto, el modelo tiene un buen balance entre precisión y recall. Si precision y recall tienen valores similares, significa que el modelo no favorece en exceso una métrica sobre la otra, es decir, no sacrifica la detección de positivos (recall) ni la precisión en sus predicciones.

5. **AUC-ROC**: 0.9415
   - El AUC-ROC (Área bajo la curva ROC) es 0.9415, lo que indica un rendimiento muy bueno del modelo (es cercano a 1) ya que la curva ROC compara la tasa de verdaderos positivos con la tasa de falsos positivos.


```{r, echo=FALSE, warning=FALSE, message=FALSE}

# 4. CALCULAR MÉTRICAS DE PERFORMANCE
accuracy <- matriz_conf$overall["Accuracy"]
precision <- matriz_conf$byClass["Precision"]
recall <- matriz_conf$byClass["Recall"]
calcular_f1_score = function(precision, recall) {
  f1_score = (2*precision*recall)/(precision + recall)
  print(sprintf("F1 score: %f", f1_score))
}


# Mostrar métricas
#cat("Accuracy:", accuracy, "\n")
#cat("Precision:", precision, "\n")
#cat("Recall:", recall, "\n")
#cat("F1-score:", f1_score, "\n")

# 5. CURVA ROC Y AUC
roc_curve <- roc(test_data$loan_status, predicciones_prob[,2])  # Segunda columna = prob de clase positiva
auc_value <- auc(roc_curve)
cat("AUC-ROC:", auc_value, "\n")

# 6. Graficar la Curva ROC
plot(roc_curve, col = "blue", main = "Figura 1: Curva ROC - Árbol de Decisión",
     xlab = "False Positive Rate", ylab = "True Positive Rate")

legend("bottomright", legend=c("ROC", "Random Classifier"), 
       col=c("blue", "gray"), lty=c(1, 2), cex=0.8)
```

En términos de la curva ROC que obtenemos, efectivamente mostramos que nuestro modelo cuenta con alta sensibilidad (True Positive Rate) y especificidad (False Positive Rate), corroborando el hecho de que la curva azul se aproxime a la esquina de la parte superior izquierda, donde el modelo clasificaría correctamente a muchos de los positivos y a los negativos. 

Si dicha curva ocupase valores más próximos a la diagonal gris, significaría que el modelo no se comporta mejor que el de una clasificación aleatoria. En el caso ideal, donde el modelo es perfecto, tendríamos la curva pegada a los ejes, de forma que obtendríamos una AUC de 1, es decir, la curva podría ir hacia el destino óptimo de manera lineal. 

Además, la curva ROC en los datos de test indica que el modelo mantiene un buen desempeño fuera de los datos de entrenamiento, lo que sugiere que no está sobreajustado. Si el modelo estuviera fuertemente sobreajustado, esperaríamos una alta performance en entrenamiento pero un deterioro significativo en test. Dado que la curva en test sigue mostrando una buena separación entre clases, se puede inferir que el modelo generaliza correctamente. En la siguiente parte, veremos cómo para mejorar el rendimiento de este modelo se podrían buscar mejores hiperparámetros que los default.


# **5. Optimización del modelo** 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(caret)
grid_search <- function(train, validation){
  maxdepth_values  <- sample(5:30, 10)
  minsplit_values  <- sample(2:20, 10) 
  minbucket_values <- sample(1:10, 10)
  
  paramgrid<- expand.grid( maxdepth_values, minsplit_values, minbucket_values)
  resultados<-data.frame(maxdepth_value = integer(), minsplit_value = integer(), minbucket_value = integer(), aucroc = numeric())
  
  for(i in maxdepth_values){
    for (j in minsplit_values ){
      for (k in minbucket_values){
      modelo <- rpart(formula = train$loan_status ~ ., data = train, method = "class", 
      control = rpart.control(maxdepth = i, minsplit = j, minbucket = k, cp = 0, xval = 0))
      predicciones_modelo <- predict(modelo, newdata = validation, type = "prob")
      roc_curve <- roc(validation$loan_status, predicciones_modelo[,2])  
      auc_value <- auc(roc_curve)
      resultados<- rbind(resultados, data.frame(maxdepth_value =i, minsplit_value = j, minbucket_value = k, aucroc = auc_value))
  }}}
  mejores_hiper <- resultados[which.max(resultados$aucroc),]

  return(mejores_hiper)
}

res <- grid_search(train_data, val_data)



modelo2 <- rpart(formula = train_data$loan_status ~ ., data = train_data, method = "class",
      control = rpart.control(maxdepth = res$maxdepth_value, minsplit = res$minsplit_value, minbucket = res$minbucket_value, cp = 0, xval = 0))
predicciones_modelo2 <- predict(modelo2, newdata = test_data, type = "prob")
roc_curve2 <- roc(test_data$loan_status, predicciones_modelo2[,2])
auc_value2 <- auc(roc_curve)

cat("Mejores Hiperparámetros:\n")
print(res)

cat("Hiperparámetros default:\n")
print(tree$control)

cat("\nValores de AUC de arbol con hiperparametros default:\n")
print(auc_value)

cat("\nValores de AUC de arbol con mejores hiperparametros:\n")
print(auc_value2)
```

